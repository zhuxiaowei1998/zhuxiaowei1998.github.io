<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistical inference on Xiaowei</title>
    <link>https://zhuxiaowei1998.github.io/tags/statistical-inference/</link>
    <description>Recent content in statistical inference on Xiaowei</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://zhuxiaowei1998.github.io/tags/statistical-inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Statistical inference 6</title>
      <link>https://zhuxiaowei1998.github.io/note/2021/12/23/statistical-inference-6/</link>
      <pubDate>Thu, 23 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhuxiaowei1998.github.io/note/2021/12/23/statistical-inference-6/</guid>
      <description>Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
 Key Element of Frequentist Theory Fundamental characteristic Explicit optimality criteria.
 Hypothesis testing: seek test which maximizes power. Point estimation: seek estimator which minimizes risk.  Classical approach Adopt the following criterion: fix a small probability $\alpha$ (known as the size) and seek a test for which
$$\mathbb{P}_{\theta}\left\{\text { Reject } H_{0}\right\} \leq \alpha \quad \text { for all } \theta \in \Theta_{0}$$</description>
    </item>
    
    <item>
      <title>Statistical inference 4</title>
      <link>https://zhuxiaowei1998.github.io/note/2021/12/22/statistical-inference-4/</link>
      <pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhuxiaowei1998.github.io/note/2021/12/22/statistical-inference-4/</guid>
      <description>Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
 Special Families of Models Exponential Families Suppose that $Y$ depends on parameter $\phi = (\phi^1,\cdots,\phi^m)^T$, to be called natural parameters, through a density of the form
$$f_{Y}(y ; \phi)=h(y) \exp \left\{s^{T} \phi-K(\phi)\right\}, \quad y \in \mathcal{Y}$$
where $\mathcal{Y}$ is a set not depending on $\phi$. Here $s \equiv s(y)=\left(s_{1}(y), \ldots, s_{m}(y)\right)^{T}$, are called natural statistics.
$$E\left(S_{i} ; \phi\right)=\frac{\partial K(\phi)}{\partial \phi^{i}}$$
$$\operatorname{cov}\left(S_{i}, S_{j} ; \phi\right)=\frac{\partial^{2} K(\phi)}{\partial \phi^{i} \partial \phi^{j}}$$</description>
    </item>
    
    <item>
      <title>Statistical inference 5</title>
      <link>https://zhuxiaowei1998.github.io/note/2021/12/22/statistical-inference-5/</link>
      <pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhuxiaowei1998.github.io/note/2021/12/22/statistical-inference-5/</guid>
      <description>Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
 Principles of Inference and Data Reduction Likelihood Likelihood function: $$L(\theta;y) = f_Y(y;\theta)$$ Log-likelihood: $$I(\theta;y) = \log f_Y(y;\theta)$$
Likelihood Principle The (strong) likelihood principle is that if $y$ and $z$ give proportional likelihood functions, the conclusions drawn from y and z should be identical, assuming adequacy of both models.
If, for all $\theta\in\Omega_\theta$, $$f_Y(y;\theta) = h(y,z)f_Z(z;\theta)$$ identical conclusions about $\theta$ should be drawn from $y$ and $z$.</description>
    </item>
    
    <item>
      <title>Statistical inference 3</title>
      <link>https://zhuxiaowei1998.github.io/note/2021/12/21/statistical-inference-3/</link>
      <pubDate>Tue, 21 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhuxiaowei1998.github.io/note/2021/12/21/statistical-inference-3/</guid>
      <description>Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
 Bayesian Methods Posterior density The posterior density of $\theta$, conditional on the observed value $Y=y$, is derived by applying Bayes&#39; rule:
$$\pi(\theta \mid y)=\frac{\pi(\theta) f(y ; \theta)}{\int_{\Omega_{\theta}} \pi\left(\theta^{\prime}\right) f\left(y ; \theta^{\prime}\right) d \theta^{\prime}}$$
$$\pi(\theta|y)\propto\pi(\theta)f(y;\theta)$$
$$\text{posterior}\propto\text{prior}\times\text{likelihood}$$
The general form of Bayes rule To minimize the Bayes risk $r(\pi,d))$ is equivalent to minimize $$\int_{\Omega_{\theta}} L(\theta, d(y)) \pi(\theta \mid y) d \theta$$
James-Stein estimator $$d^{a}(Y)=\left(1-\frac{a}{\|Y\|^{2}}\right) Y$$</description>
    </item>
    
    <item>
      <title>Statistical inference 1</title>
      <link>https://zhuxiaowei1998.github.io/note/2021/12/20/statistical-inference-1/</link>
      <pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhuxiaowei1998.github.io/note/2021/12/20/statistical-inference-1/</guid>
      <description>Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
 Approaches to Statistical Inference What is statistical inference? Observational data modelled as observed values of random variables, to provide framework from which inductive conclusions may be drawn about mechanism giving rise to data.
Types of inference  Point estimation Confidence set estimation Hypothesis testing  Three paradigms of inference  Bayesian Fisherian frequentist  Bayesian inference Unknown parameter $\theta$ treated as random variable.</description>
    </item>
    
    <item>
      <title>Statistical inference 2</title>
      <link>https://zhuxiaowei1998.github.io/note/2021/12/20/statistical-inference-2/</link>
      <pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhuxiaowei1998.github.io/note/2021/12/20/statistical-inference-2/</guid>
      <description>Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
 Decision Theory Formulation Elements of a formal decision problem:
 Parameter space. $\Omega_\theta$ Sample space $\mathcal{Y}$. $y = (y_1,\cdots,y_n)\in\mathbb{R}^n$ Family of distributions. $\{\mathbb{P}_\theta(y),y\in\mathcal{Y},\theta\in\Omega_\theta\}$ Action space $\mathcal{A}$. Loss function $L$. Function $L:\Omega_\theta\times\mathcal{A}\rightarrow\mathbb{R}$. Decision rule $d$. Function $d:\mathcal{Y}\rightarrow\mathcal{A}$.  Risk Function $$R(\theta, d)=\mathbb{E}_{\theta} L(\theta, d(Y))=\int_{\mathcal{Y}} L(\theta, d(y)) f(y ; \theta) d y$$
Common loss function Squared error loss function: $$L(\theta,a) = (\theta-a)^2$$ Absolute error loss function: $$L(\theta,a) = |\theta-a|$$</description>
    </item>
    
  </channel>
</rss>
