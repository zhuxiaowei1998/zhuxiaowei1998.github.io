<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Statistical inference 6 - Xiaowei</title>
    <meta property="og:title" content="Statistical inference 6 - Xiaowei">
    
    <meta name="twitter:card" content="summary">

    
      
    

    
      
      <meta property="description" content="Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
[&amp;hellip;] Explicit optimality criteria.
[&amp;hellip;] Adopt the following criterion: fix a small probability $\alpha$ &amp;hellip;">
      <meta property="og:description" content="Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
[&amp;hellip;] Explicit optimality criteria.
[&amp;hellip;] Adopt the following criterion: fix a small probability $\alpha$ &amp;hellip;">
      
    

    
    

    

    
    

    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="/css/custom.css" />

  </head>

  
  <body class="note">
    <header class="masthead">
      <h1><a href="/">Xiaowei</a></h1>

<p class="tagline">An unexamined life is not worth living.</p>

      <nav class="menu">
  <input id="menu-check" type="checkbox" hidden/>
  <label id="menu-label" for="menu-check" class="unselectable" hidden>
    <span class="icon close-icon">✕</span>
    <span class="icon open-icon">☰</span>
    <span class="text">Menu</span>
  </label>
  <ul>
  
  
  <li><a href="/">Home</a></li>
  
  <li><a href="/about/">About</a></li>
  
  <li><a href="/categories/">Categories</a></li>
  
  <li><a href="/tags/">Tags</a></li>
  
  <li><a href="/links/">Links</a></li>
  
  
  </ul>
</nav>

    </header>

    <article class="main">
      <header class="title">
      
<h1>Statistical inference 6</h1>

<h3>Xiaowei Zhu
  /  2021-12-23</h3>
<hr>


      </header>





<blockquote>
<p>Reference: Fundamentals of Statistical Inference&mdash;&mdash;G.Alastair Young</p>
</blockquote>
<h1 id="key-element-of-frequentist-theory">Key Element of Frequentist Theory</h1>
<h2 id="fundamental-characteristic">Fundamental characteristic</h2>
<p>Explicit optimality criteria.</p>
<ul>
<li>Hypothesis testing: seek test which maximizes power.</li>
<li>Point estimation: seek estimator which minimizes risk.</li>
</ul>
<h2 id="classical-approach">Classical approach</h2>
<p>Adopt the following criterion: fix a small probability <code>$\alpha$</code> (known as
the size) and seek a test for which</p>
<p><code>$$\mathbb{P}_{\theta}\left\{\text { Reject } H_{0}\right\} \leq \alpha \quad \text { for all } \theta \in \Theta_{0}$$</code></p>
<h2 id="test-functions">Test functions</h2>
<p>Conventional formulation: choose a test statistic <code>$t(Y)$</code> with distribution depending on <code>$\theta$</code> and a critical region <code>$C_\alpha$</code>, reject <code>$H_0$</code> based on <code>$Y = y$</code> iff <code>$t(y)\in C_\alpha$</code>.</p>
<p>Slight reformulation: define the test function <code>$\phi(y)$</code> by
<code>$$\phi(y)= \begin{cases}1 &amp; \text { if } t(y) \in C_{\alpha} \\ 0 &amp; \text { otherwise }\end{cases}$$</code></p>
<h2 id="power">Power</h2>
<p>Criterion for deciding whether one test is better than another:power.</p>
<p>The power function of a test <code>$\phi$</code> is defined to be</p>
<p><code>$$w(\theta)=\mathbb{P}_{\theta}\left\{\text { Reject } H_{0}\right\}=\mathbb{E}_{\theta}\{\phi(Y)\}$$</code></p>
<p>defined for all <code>$\theta\in\Omega_\theta$</code>.</p>
<h2 id="ump-tests">UMP Tests</h2>
<p>A uniformly most powerful or UMP test of size <code>$\alpha$</code> is a test <code>$\phi_0(\cdot)$</code> for which</p>
<ul>
<li><code>$\mathbb{E}_{\theta} \phi_{0}(Y) \leq \alpha \text { for all } \theta \in \Theta_{0}$</code></li>
<li>Given any other test <code>$\phi(\cdot)$</code> for which <code>$\mathbb{E}_{\theta} \phi(Y) \leq \alpha$</code> for all <code>$\theta\in \Theta_0$</code>, we have <code>$\mathbb{E}_{\theta} \phi_{0}(Y) \geq  \mathbb{E}_{\theta} \phi(Y)$</code> for all <code>$\theta\in\Theta_1$</code></li>
</ul>
<h2 id="monotone-likelihood-ratio">Monotone likelihood ratio</h2>
<p>The family of densities <code>$\left\{f(y ; \theta), \theta \in \Omega_{\theta} \subseteq \mathbb{R}\right\}$</code> with real scalar parameter <code>$\theta$</code> is said to be of monotone likelihood ratio(MLR) if
there exists a function <code>$t(y)$</code> such that the likelihood ratio</p>
<p><code>$$\frac{f(y;\theta_2)}{f(y;\theta_1)}$$</code></p>
<p>is a non-decreasing function of <code>$t(y)$</code> whenever <code>$\theta_1\leq\theta_2$</code></p>
<h2 id="unbiasedness-and-umpu-tests">Unbiasedness and UMPU tests</h2>
<p>A test <code>$\phi$</code> of <code>$H_0$</code> : <code>\theta\in\Theta_0</code> against <code>$H_1:\theta\in\Theta_1$</code> is unbiased of size <code>$\alpha$</code> if 
<code>$$\sup _{\theta \in \Theta_{0}} \mathbb{E}_{\theta}\{\phi(Y)\}=\alpha$$</code>
and 
<code>$$\mathbb{E}_{\theta}\{\phi(Y)\} \geq \alpha \text { for all } \theta \in \Theta_{1}$$</code>
A test which is uniformly most powerful amongst the class of all unbiased tests is uniformly most powerful unbiased, abbreviated UMPU.</p>
<h2 id="conditionality-principle">Conditionality Principle</h2>
<p>The minimal sufficient statistic for <code>$\theta$</code> is <code>$(Y, \delta)$</code>, and <code>$\delta$</code> has a distribution not depending on <code>$\theta$</code>.</p>
<p>The Conditionality Principle argues that inference about <code>\theta</code> should be based on the conditional distribution of <code>$Y$</code> given <code>$\delta$</code>.</p>
<h2 id="similar-tests">Similar tests</h2>
<p>Suppose we have <code>$\theta = (\psi,\lambda)$</code>, with <code>$\psi$</code> the interest parameter and <code>$\lambda$</code>
a nuisance parameter. Suppose the minimal sufficient statistic <code>$T = (S, C)$</code>, where the conditional distribution of <code>$S$</code> given <code>$C = c$</code>
depends on <code>$\psi$</code>, but not <code>$\lambda$</code>, for each <code>$c$</code>.</p>
<p>Test using the conditional distribution of <code>$S$</code> given <code>$C$</code>. This eliminates the nuisance parameter: the test is similar.</p>
<h2 id="similarity-definition">Similarity: definition</h2>
<p>Suppose <code>$\theta = (\psi,\lambda)$</code> and the parameter space is of the form <code>$\Theta = \Psi \times \Lambda$</code>. Suppose we wish to test the null hypothesis <code>$H_0 : \psi = \psi_0$</code> against the alternative <code>$H_1 : \psi \neq \psi_0$</code>, with <code>$\lambda$</code> treated as a nuisance parameter.</p>
<p>Suppose <code>$\phi(y)$</code>, <code>$y\in\mathcal{Y}$</code> is a test of size α for which
<code>$$\mathbb{E}_{\psi_{0}, \lambda}\{\phi(X)\}=\alpha \text { for all } \lambda \in \Lambda$$</code>
Then <code>$\psi$</code> is called a similar test of size <code>$\alpha$</code>.</p>
<h2 id="theorem">Theorem</h2>
<p>To estimate a real-valued parameter <code>$\theta$</code> with an estimator <code>$d(Y)$</code> say.The loss function <code>$L(\theta, d)$</code> is a convex function of <code>$d$</code> for each <code>$\theta$</code>.</p>
<p>Let <code>$d_1(Y)$</code> be an unbiased estimator for <code>$T$</code> and suppose <code>$T$</code> is a sufficient statistic. Then the estimator</p>
<p><code>$$\chi(T)=\mathbb{E}\left\{d_{1}(Y) \mid T\right\}$$</code></p>
<p>is also unbiased and has risk not exceeding that of <code>$d_1$</code>.</p>
<h2 id="a-general-result">A general result</h2>
<p>Theorem. Let <code>$T$</code> be a complete sufficient statistic for a (vector)
parameter <code>$\theta$</code>, and let <code>$\phi(T)$</code> be any estimator based only on <code>$T$</code>. Then <code>$\phi(T)$</code> is the UMVUE of its expectation.</p>
<h2 id="bayes-factors">Bayes factors</h2>
<p>Posterior Odds = Prior Odds × Bayes Factor.</p>
<p>The Bayes factor in this case is defined as
<code>$$B=\frac{\int_{\Theta_{0}} f(x ; \theta) g_{0}(\theta) d \theta}{\int_{\Theta_{1}} f(x ; \theta) g_{1}(\theta) d \theta}$$</code></p>
<h2 id="bic">BIC</h2>
<p>One method which can provide a rough measure of the evidence in favour of one model over another without reference to any prior distribution is provided by the Bayesian Information Criterion(BIC).</p>
<p><code>$$\Delta B I C=W-\left(p_{2}-p_{1}\right) \log n$$</code></p>
<p>where <code>$p_i$</code> is the number of parameters in model <code>$M_i</code>, <code>$i=1,2</code>, and <code>$W$</code> is the likelihood ratio statistic
<code>$$W=-2 \log \frac{\sup _{\theta_{1}} f\left(x ; \theta_{1}, M_{1}\right)}{\sup _{\theta_{2}} f\left(x ; \theta_{2}, M_{2}\right)}$$</code></p>
<h2 id="confidence-sets">Confidence sets</h2>
<ul>
<li>Via pivotal quantity.</li>
<li>Inversion of hypothesis test.</li>
</ul>
<h2 id="pivotal-quantities">Pivotal quantities</h2>
<p>A random function <code>$T(Y, \theta)$</code> is said to be pivotal, or a pivotal quantity, if it is a function of both <code>$Y$</code> and <code>$\theta$</code> whose distribution does not depend on <code>\theta</code>.</p>
<p>Given <code>$\alpha\in(0, 1)$</code> and a pivotal quantity <code>$T(Y, \theta)$</code>, we can find constants <code>$c_1$</code> and <code>$c_2$</code> such that</p>
<p><code>$$\operatorname{Pr}_{\theta}\left\{c_{1} \leq T(Y, \theta) \leq c_{2}\right\}=1-\alpha \text { for all } \theta$$</code></p>
<p>Provided <code>$T(Y, \theta)$</code> is of a reasonably manageable functional form, we can rewrite this relationship in the form
<code>$$\operatorname{Pr}_{\theta}\{L(Y) \leq \theta \leq U(Y)\}=1-\alpha \text { for all } \theta$$</code></p>



  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/note/2021/12/23/diary/">日记</a></span>
  <span class="nav-next"><a href="/note/2021/12/24/diary/">日记</a> &rarr;</span>
</nav>





<script src="//yihui.org/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.org/js/center-img.js"></script>

  

  
  <hr>
  <div class="copyright">© <a href="https://zhuxiaowei1998.github.io/">Xiaowei Zhu</a> 2021</div>
  
  </footer>
  </article>
  
  </body>
</html>

