<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics on Xiaowei</title>
    <link>https://zhuxiaowei1998.github.io/categories/statistics/</link>
    <description>Recent content in statistics on Xiaowei</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://zhuxiaowei1998.github.io/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Statistical inference 3</title>
      <link>https://zhuxiaowei1998.github.io/note/2021/12/21/statistical-inference-3/</link>
      <pubDate>Tue, 21 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhuxiaowei1998.github.io/note/2021/12/21/statistical-inference-3/</guid>
      <description>Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
 Bayesian Methods Posterior density The posterior density of $\theta$, conditional on the observed value $Y=y$, is derived by applying Bayes&#39; rule:
$$\pi(\theta \mid y)=\frac{\pi(\theta) f(y ; \theta)}{\int_{\Omega_{\theta}} \pi\left(\theta^{\prime}\right) f\left(y ; \theta^{\prime}\right) d \theta^{\prime}}$$
$$\pi(\theta|y)\propto\pi(\theta)f(y;\theta)$$
$$\text{posterior}\propto\text{prior}\times\text{likelihood}$$
The general form of Bayes rule To minimize the Bayes risk $r(\pi,d))$ is equivalent to minimize $$\int_{\Omega_{\theta}} L(\theta, d(y)) \pi(\theta \mid y) d \theta$$
James-Stein estimator $$d^{a}(Y)=\left(1-\frac{a}{\|Y\|^{2}}\right) Y$$</description>
    </item>
    
    <item>
      <title>Statistical inference 1</title>
      <link>https://zhuxiaowei1998.github.io/note/2021/12/20/statistical-inference-1/</link>
      <pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhuxiaowei1998.github.io/note/2021/12/20/statistical-inference-1/</guid>
      <description>Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
 Approaches to Statistical Inference What is statistical inference? Observational data modelled as observed values of random variables, to provide framework from which inductive conclusions may be drawn about mechanism giving rise to data.
Types of inference  Point estimation Confidence set estimation Hypothesis testing  Three paradigms of inference  Bayesian Fisherian frequentist  Bayesian inference Unknown parameter $\theta$ treated as random variable.</description>
    </item>
    
    <item>
      <title>Statistical inference 2</title>
      <link>https://zhuxiaowei1998.github.io/note/2021/12/20/statistical-inference-2/</link>
      <pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhuxiaowei1998.github.io/note/2021/12/20/statistical-inference-2/</guid>
      <description>Reference: Fundamentals of Statistical Inference&amp;mdash;&amp;mdash;G.Alastair Young
 Decision Theory Formulation Elements of a formal decision problem:
 Parameter space. $\Omega_\theta$ Sample space $\mathcal{Y}$. $y = (y_1,\cdots,y_n)\in\mathbb{R}^n$ Family of distributions. $\{\mathbb{P}_\theta(y),y\in\mathcal{Y},\theta\in\Omega_\theta\}$ Action space $\mathcal{A}$. Loss function $L$. Function $L:\Omega_\theta\times\mathcal{A}\rightarrow\mathbb{R}$. Decision rule $d$. Function $d:\mathcal{Y}\rightarrow\mathcal{A}$.  Risk Function $$R(\theta, d)=\mathbb{E}_{\theta} L(\theta, d(Y))=\int_{\mathcal{Y}} L(\theta, d(y)) f(y ; \theta) d y$$
Common loss function Squared error loss function: $$L(\theta,a) = (\theta-a)^2$$ Absolute error loss function: $$L(\theta,a) = |\theta-a|$$</description>
    </item>
    
  </channel>
</rss>
